{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ee5ad761-1a7c-4cd4-9978-cc8b28d774d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Overview of Managed Connectors in Databricks Lakeflow Connect\n",
    "\n",
    "Databricks Lakeflow Connect provides managed connectors for ingesting data from SaaS applications and databases. The resulting ingestion pipeline is:\n",
    "\n",
    "- **Governed by Unity Catalog**\n",
    "- **Powered by serverless compute and Delta Live Tables (DLT)**\n",
    "\n",
    "Managed connectors leverage efficient incremental reads and writes to make data ingestion **faster, scalable, and more cost-efficient**, while keeping your data **fresh for downstream consumption**.\n",
    "\n",
    "---\n",
    "\n",
    "## SaaS Connector Components\n",
    "\n",
    "A SaaS connector is modeled by the following components:\n",
    "\n",
    "- **Connection**: A Unity Catalog securable object that stores authentication details for the application.\n",
    "- **Ingestion pipeline**: Ingests the staged data into Delta tables. This component is modeled as a **serverless DLT pipeline**.\n",
    "\n",
    "### SaaS Connector Components Diagram\n",
    "\n",
    "![SaaS Connector Components](https://docs.databricks.com/aws/en/assets/images/saas-connector-components-799859c34970b8f86c6758f7ea510233.png)\n",
    "\n",
    "---\n",
    "\n",
    "## Database Connector Components\n",
    "\n",
    "A database connector is modeled by the following components:\n",
    "\n",
    "- **Connection**: A Unity Catalog securable object that stores authentication details for the database.\n",
    "- **Gateway**: Extracts data from the source database and maintains the integrity of transactions during the transfer. For cloud-based databases, the gateway is configured as a **DLT pipeline with classic compute**.\n",
    "- **Staging storage**: A Unity Catalog volume where data from the gateway is staged before being applied to a Delta table. The staging storage account is created when you deploy the gateway and exists within the catalog and schema that you specify.\n",
    "- **Ingestion pipeline**: Ingests the staged data into Delta tables. This component is modeled as a **serverless DLT pipeline**.\n",
    "\n",
    "![Database Connector Components](https://docs.databricks.com/aws/en/assets/images/database-connector-components-cceed910a6b9ab41d2348ee68fd361df.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "033fecb7-eb2d-48ff-a510-9e09e35cb4b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Managed SaaS connectors support column-level selection (API-only)\n",
    "\n",
    "Managed connectors for enterprise applications (such as Salesforce, Workday, and ServiceNow) now support column-level selection and deselection. You can also specify whether to automatically ingest future columns as they're added to the source."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fdd3a37b-96c4-405f-a9b2-a26ece59a71f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### New Feature : Configure Ingestion with Column Selection and include new columns\n",
    "\n",
    "It can be setup under the pipelines Lakeflow UI by selecting the data source\n",
    "\n",
    "![Lakeflow UI](https://www.databricks.com/sites/default/files/inline-images/lakeflow-connect-video.gif?v=1718218999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47635c76-5047-44bb-8c0b-c6a1ef012b32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "INSTANCE = \"https://<databricks-instance>\"\n",
    "TOKEN = \"<your-databricks-pat>\"\n",
    "CONNECTION_ID = \"<your-connection-id>\"\n",
    "\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {TOKEN}\",\n",
    "    \"Content-Type\": \"application/json\"\n",
    "}\n",
    "\n",
    "# Example: Ingest only specific columns from \"Account\" table in Salesforce\n",
    "payload = {\n",
    "    \"connection_id\": CONNECTION_ID,\n",
    "    \"source\": {\n",
    "        \"object\": \"Account\",\n",
    "        \"columns\": [\n",
    "            {\"name\": \"Id\"},\n",
    "            {\"name\": \"Name\"},\n",
    "            {\"name\": \"Industry\"}\n",
    "        ],\n",
    "        \"include_new_columns\": False  # set to True to auto-ingest new columns in future\n",
    "    },\n",
    "    \"target\": {\n",
    "        \"schema_name\": \"salesforce\",\n",
    "        \"table_name\": \"account_selected_columns\"\n",
    "    },\n",
    "    \"ingestion_type\": \"full\"  # or \"incremental\"\n",
    "}\n",
    "\n",
    "response = requests.post(\n",
    "    f\"{INSTANCE}/api/2.0/lakeflow/ingestions\",\n",
    "    headers=headers,\n",
    "    json=payload\n",
    ")\n",
    "\n",
    "print(response.status_code)\n",
    "print(response.json())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "47dc8661-7816-4f5b-8355-2312020bd690",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# ðŸ“Œ Notes\n",
    "\n",
    "- `include_new_columns: true` automatically ingests newly added fields later.\n",
    "\n",
    "### âœ… This works for connectors like:\n",
    "- Salesforce  \n",
    "- Workday  \n",
    "- ServiceNow  \n",
    "\n",
    "You can call this API repeatedly to:\n",
    "- Create ingestions for multiple tables  \n",
    "- Tweak column selections for each table  \n",
    "\n",
    "If successful, the ingestion appears in the **Lakeflow UI** under:  \n",
    "**`Connect > Ingestions`**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5f9a5f8-40d0-4e23-83b0-6afdbea7b9d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###Recommended for downstream processing \n",
    "- For Databricks Jobs (without DLT): You can handle schema drift by enabling .option(\"mergeSchema\", \"true\") when writing to a Delta table â€” this allows new columns to be automatically added.\n",
    "\n",
    "- For DLT Pipelines: DLT supports schema drift out-of-the-box with features like expectations, table() definitions, and Unity Catalog governance, making it easier to manage evolving schemas with built-in logging and monitoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cfbc83ba-68fd-4283-b7f9-1bd17a3430e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Lakeflow connect",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}